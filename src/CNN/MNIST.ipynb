{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras \n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras import backend as k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While proceeding further, `img_rows` and `img_cols` are used as the image dimensions. In MNIST dataset, it is 28 x 28. We also need to check the data format i.e. `channels_first` or `channels_last`. In CNN, we can normalize data before hands such that large terms of the calculations can be reduced to smaller terms. Like, we can normalize the `x_train` and `x_test` data by dividing it by 255.\n",
    "\n",
    "Checking data-format: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols=28, 28\n",
    " \n",
    "if k.image_data_format() == 'channels_first':\n",
    "   x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "   x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "   inpx = (1, img_rows, img_cols)\n",
    " \n",
    "else:\n",
    "   x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "   x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "   inpx = (img_rows, img_cols, 1)\n",
    " \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the working of each layer in the CNN model:\n",
    "- layer1 : the Conv2d layer which convolves the image using 32 filters each of size (3*3). \n",
    "- layer2 : again a Conv2D layer which : also used to convolve the image and : using 64 filters each of size (3*3). \n",
    "- layer3 : the MaxPooling2D layer which picks the max value out of a matrix of size (3*3). \n",
    "- layer4 : showing Dropout at a rate of 0.5. \n",
    "- layer5 : flattening the output obtained from layer4 and th: flattens output : passed to layer6. \n",
    "- layer6 : a hidden layer of a neural network containing 250 neurons. \n",
    "- layer7 : the output layer having 10 neurons for 10 classes of output that : using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpx = Input(shape=inpx)\n",
    "layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)\n",
    "layer2 = Conv2D(64, (3, 3), activation='relu')(layer1)\n",
    "layer3 = MaxPooling2D(pool_size=(3, 3))(layer2)\n",
    "layer4 = Dropout(0.5)(layer3)\n",
    "layer5 = Flatten()(layer4)\n",
    "layer6 = Dense(250, activation='sigmoid')(layer5)\n",
    "layer7 = Dense(10, activation='softmax')(layer6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling compile and fit function: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 445ms/step - accuracy: 0.1013 - loss: 2.5401\n",
      "Epoch 2/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 455ms/step - accuracy: 0.1021 - loss: 2.5221\n",
      "Epoch 3/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 391ms/step - accuracy: 0.0999 - loss: 2.5019\n",
      "Epoch 4/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 426ms/step - accuracy: 0.1003 - loss: 2.4869\n",
      "Epoch 5/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 416ms/step - accuracy: 0.1039 - loss: 2.4653\n",
      "Epoch 6/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 444ms/step - accuracy: 0.1029 - loss: 2.4478\n",
      "Epoch 7/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 497ms/step - accuracy: 0.1027 - loss: 2.4358\n",
      "Epoch 8/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 457ms/step - accuracy: 0.1009 - loss: 2.4179\n",
      "Epoch 9/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 400ms/step - accuracy: 0.1017 - loss: 2.4039\n",
      "Epoch 10/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 402ms/step - accuracy: 0.1026 - loss: 2.3894\n",
      "Epoch 11/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 393ms/step - accuracy: 0.1030 - loss: 2.3747\n",
      "Epoch 12/12\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 423ms/step - accuracy: 0.1008 - loss: 2.3615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27bf388c590>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([inpx], layer7)\n",
    "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=12, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate function: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss= 2.350484848022461\n",
      "Accuracy= 0.10100000351667404\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Loss=', score[0])\n",
    "print('Accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
